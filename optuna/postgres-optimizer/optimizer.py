#!/usr/bin/env python3
"""
Multi-Cloud Postgres Configuration Optimizer using Bayesian Optimization (Optuna).

Supports two optimization modes:
- infra: Tune VM specs (CPU, RAM, disk) - creates new VM per trial
- config: Tune postgresql.conf on fixed host - reconfigures existing VM

Usage:
    # Infrastructure optimization (tune VM specs)
    uv run python postgres-optimizer/optimizer.py --cloud timeweb --mode infra --trials 10

    # Config optimization on fixed host (faster, more trials)
    uv run python postgres-optimizer/optimizer.py --cloud timeweb --mode config --cpu 8 --ram 32 --trials 50

    # Full optimization (infra first, then config on best host)
    uv run python postgres-optimizer/optimizer.py --cloud timeweb --mode full --trials 20

    # Show results / export
    uv run python postgres-optimizer/optimizer.py --cloud timeweb --show-results
    uv run python postgres-optimizer/optimizer.py --cloud timeweb --export-md
"""

import argparse
import json
import re
import sys
import time
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from pathlib import Path

import optuna
from optuna.samplers import TPESampler

# Add parent dir to path for common imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from common import (
    destroy_all,
    get_terraform,
    get_tf_output,
    load_results,
    run_ssh_command,
    save_results,
    wait_for_vm_ready,
)

from cloud_config import (
    CloudConfig,
    get_cloud_config,
    get_config_search_space,
    get_infra_search_space,
)

RESULTS_DIR = Path(__file__).parent
STUDY_DB = RESULTS_DIR / "study.db"


class Mode(Enum):
    """Optimization mode."""

    INFRA = "infra"  # Tune VM specs (CPU, RAM, disk)
    CONFIG = "config"  # Tune postgresql.conf (fixed host)
    FULL = "full"  # Both phases


# Available optimization metrics
METRICS = {
    "tps": "Transactions per second (higher is better)",
    "latency_avg_ms": "Average latency in ms (lower is better)",
    "cost_efficiency": "TPS per $/hr (higher is better)",
}


@dataclass
class PgBenchResult:
    """pgbench benchmark results."""

    tps: float = 0.0
    latency_avg_ms: float = 0.0
    latency_stddev_ms: float = 0.0
    transactions: int = 0
    duration_s: float = 0.0
    error: str | None = None


@dataclass
class BenchmarkResult:
    """Full benchmark result with config."""

    infra_config: dict
    pg_config: dict
    tps: float = 0.0
    latency_avg_ms: float = 0.0
    latency_stddev_ms: float = 0.0
    duration_s: float = 0.0
    error: str | None = None


def results_file(cloud: str, mode: str) -> Path:
    """Get results file path for a cloud and mode."""
    return RESULTS_DIR / f"results_{cloud}_{mode}.json"


def config_to_key(infra: dict, pg_config: dict) -> str:
    """Convert config dicts to a hashable key for deduplication."""
    return json.dumps({"infra": infra, "pg": pg_config}, sort_keys=True)


def find_cached_result(
    infra: dict, pg_config: dict, cloud: str, mode: str
) -> dict | None:
    """Find a cached successful result for the given config."""
    target_key = config_to_key(infra, pg_config)
    for result in load_results(results_file(cloud, mode)):
        result_key = config_to_key(
            result.get("infra_config", {}), result.get("pg_config", {})
        )
        if result_key == target_key:
            if result.get("error"):
                return None
            if result.get("tps", 0) <= 0:
                return None
            return result
    return None


def generate_postgresql_conf(pg_config: dict, ram_gb: int) -> str:
    """Generate postgresql.conf tuning content."""
    # Calculate memory values
    shared_buffers_mb = int(ram_gb * 1024 * pg_config["shared_buffers_pct"] / 100)
    effective_cache_size_mb = int(
        ram_gb * 1024 * pg_config["effective_cache_size_pct"] / 100
    )

    return f"""# Auto-generated by postgres-optimizer
# RAM: {ram_gb}GB

# Memory
shared_buffers = {shared_buffers_mb}MB
effective_cache_size = {effective_cache_size_mb}MB
work_mem = {pg_config["work_mem_mb"]}MB
maintenance_work_mem = {pg_config["maintenance_work_mem_mb"]}MB

# Connections
max_connections = {pg_config["max_connections"]}

# Planner
random_page_cost = {pg_config["random_page_cost"]}
effective_io_concurrency = {pg_config["effective_io_concurrency"]}

# WAL
wal_buffers = {pg_config["wal_buffers_mb"]}MB
max_wal_size = {pg_config["max_wal_size_gb"]}GB
checkpoint_completion_target = {pg_config["checkpoint_completion_target"]}

# Workers
max_worker_processes = {pg_config["max_worker_processes"]}
max_parallel_workers_per_gather = {pg_config["max_parallel_workers_per_gather"]}
max_parallel_workers = {pg_config["max_worker_processes"]}
"""


def reconfigure_postgres(
    vm_ip: str, pg_config: dict, ram_gb: int, mode: str = "single"
) -> bool:
    """Reconfigure Postgres with new settings (no VM recreation)."""
    print(f"  Reconfiguring Postgres ({mode} mode) with: {pg_config}")

    if mode == "cluster":
        return reconfigure_patroni(vm_ip, pg_config, ram_gb)
    else:
        return reconfigure_postgres_single(vm_ip, pg_config, ram_gb)


def reconfigure_postgres_single(vm_ip: str, pg_config: dict, ram_gb: int) -> bool:
    """Reconfigure single Postgres node via conf.d."""
    config_content = generate_postgresql_conf(pg_config, ram_gb)

    # Upload config (PostgreSQL 18)
    upload_cmd = f"cat > /etc/postgresql/18/main/conf.d/tuning.conf << 'EOF'\n{config_content}\nEOF"
    code, output = run_ssh_command(vm_ip, upload_cmd, timeout=30)
    if code != 0:
        print(f"  Failed to upload config: {output}")
        return False

    # Restart Postgres
    restart_cmd = "systemctl restart postgresql && sleep 3 && pg_isready"
    code, output = run_ssh_command(vm_ip, restart_cmd, timeout=60)
    if code != 0:
        print(f"  Failed to restart Postgres: {output}")
        return False

    print("  Postgres reconfigured successfully")
    return True


def reconfigure_patroni(vm_ip: str, pg_config: dict, ram_gb: int) -> bool:
    """Reconfigure Patroni cluster via patronictl."""
    # Generate config values
    shared_buffers = int(ram_gb * 1024 * pg_config["shared_buffers_pct"] / 100)
    effective_cache_size = int(
        ram_gb * 1024 * pg_config["effective_cache_size_pct"] / 100
    )

    # Build patronictl edit-config command (applies to all nodes via DCS)
    patch_json = json.dumps(
        {
            "postgresql": {
                "parameters": {
                    "shared_buffers": f"{shared_buffers}MB",
                    "effective_cache_size": f"{effective_cache_size}MB",
                    "work_mem": f"{pg_config['work_mem_mb']}MB",
                    "maintenance_work_mem": f"{pg_config['maintenance_work_mem_mb']}MB",
                    "max_connections": pg_config["max_connections"],
                    "random_page_cost": pg_config["random_page_cost"],
                    "effective_io_concurrency": pg_config["effective_io_concurrency"],
                    "wal_buffers": f"{pg_config['wal_buffers_mb']}MB",
                    "max_wal_size": f"{pg_config['max_wal_size_gb']}GB",
                    "checkpoint_completion_target": pg_config[
                        "checkpoint_completion_target"
                    ],
                    "max_worker_processes": pg_config["max_worker_processes"],
                    "max_parallel_workers_per_gather": pg_config[
                        "max_parallel_workers_per_gather"
                    ],
                    "max_parallel_workers": pg_config["max_worker_processes"],
                }
            }
        }
    )

    # Apply config change via patronictl
    edit_cmd = f"patronictl -c /etc/patroni/patroni.yml edit-config --apply '{patch_json}' --force"
    code, output = run_ssh_command(vm_ip, edit_cmd, timeout=60)
    if code != 0:
        print(f"  Failed to update Patroni config: {output}")
        return False

    # Restart cluster to apply pending changes
    restart_cmd = (
        "patronictl -c /etc/patroni/patroni.yml restart postgres-cluster --force"
    )
    code, output = run_ssh_command(vm_ip, restart_cmd, timeout=120)
    if code != 0:
        print(f"  Warning: Patroni restart returned non-zero: {output}")

    # Wait for cluster to be healthy
    time.sleep(10)
    check_cmd = "patronictl -c /etc/patroni/patroni.yml list"
    code, output = run_ssh_command(vm_ip, check_cmd, timeout=30)
    if "Leader" in output:
        print("  Patroni cluster reconfigured successfully")
        return True

    print(f"  Warning: Cluster state after restart: {output}")
    return True  # Continue anyway


def ensure_infra(
    cloud_config: CloudConfig, infra_config: dict | None = None
) -> tuple[str, str]:
    """Ensure Postgres and Benchmark VMs exist. Returns (benchmark_ip, postgres_ip)."""
    print(f"\nChecking infrastructure for {cloud_config.name}...")

    tf = get_terraform(cloud_config.terraform_dir)
    mode = infra_config.get("mode", "single") if infra_config else "single"

    postgres_ip = get_tf_output(tf, "postgres_vm_ip")
    benchmark_ip = get_tf_output(tf, "benchmark_vm_ip")

    if postgres_ip and benchmark_ip:
        print(f"  Found Postgres VM: {postgres_ip}")
        print(f"  Found Benchmark VM: {benchmark_ip}")
        try:
            code, _ = run_ssh_command(postgres_ip, "pg_isready", timeout=10)
            if code == 0:
                return benchmark_ip, postgres_ip
        except Exception:
            pass

    print("  Creating infrastructure...")
    tf_vars = {
        "postgres_enabled": True,
        "postgres_mode": mode,
        "redis_enabled": False,
        "minio_enabled": False,
    }

    if infra_config:
        tf_vars.update(
            {
                "postgres_cpu": infra_config.get("cpu", 4),
                "postgres_ram_gb": infra_config.get("ram_gb", 16),
                "postgres_disk_type": infra_config.get("disk_type", "nvme"),
                "postgres_disk_size_gb": infra_config.get("disk_size_gb", 100),
            }
        )

    ret_code, stdout, stderr = tf.apply(skip_plan=True, var=tf_vars)

    if ret_code != 0:
        raise RuntimeError(f"Failed to create infrastructure: {stderr}")

    postgres_ip = get_tf_output(tf, "postgres_vm_ip")
    benchmark_ip = get_tf_output(tf, "benchmark_vm_ip")

    if not postgres_ip:
        raise RuntimeError("Postgres VM created but no IP returned")
    if not benchmark_ip:
        raise RuntimeError("Benchmark VM created but no IP returned")

    print(f"  Postgres VM: {postgres_ip}")
    print(f"  Benchmark VM: {benchmark_ip}")

    wait_for_vm_ready(postgres_ip)
    wait_for_vm_ready(benchmark_ip)

    # Wait for Postgres to be ready
    if mode == "cluster":
        wait_for_patroni_ready(postgres_ip)
    else:
        wait_for_postgres_ready(postgres_ip)

    return benchmark_ip, postgres_ip


def wait_for_patroni_ready(vm_ip: str, timeout: int = 300) -> bool:
    """Wait for Patroni cluster to be ready with a primary."""
    print("  Waiting for Patroni cluster to elect a primary...")

    start = time.time()
    while time.time() - start < timeout:
        try:
            # Check Patroni REST API
            code, output = run_ssh_command(
                vm_ip,
                "curl -s http://localhost:8008/leader 2>/dev/null || curl -s http://localhost:8008/ 2>/dev/null",
                timeout=10,
            )
            if code == 0 and (
                "running" in output.lower() or "leader" in output.lower()
            ):
                # Also verify pg_isready
                code2, _ = run_ssh_command(vm_ip, "pg_isready -h 127.0.0.1", timeout=10)
                if code2 == 0:
                    print(f"  Patroni cluster ready! ({time.time() - start:.0f}s)")
                    return True
        except Exception:
            pass
        time.sleep(10)

    print(f"  Warning: Patroni cluster not ready after {timeout}s")
    return False


def wait_for_postgres_ready(vm_ip: str, timeout: int = 180) -> bool:
    """Wait for Postgres to be ready."""
    print("  Waiting for Postgres to be ready...")

    start = time.time()
    while time.time() - start < timeout:
        try:
            code, output = run_ssh_command(vm_ip, "pg_isready", timeout=10)
            if code == 0:
                print(f"  Postgres is ready! ({time.time() - start:.0f}s)")
                return True
        except Exception:
            pass
        time.sleep(5)

    print(f"  Warning: Postgres not ready after {timeout}s")
    return False


def initialize_pgbench(postgres_ip: str, scale: int = 100) -> bool:
    """Initialize pgbench database on Postgres VM."""
    print(f"  Initializing pgbench (scale={scale}) on Postgres VM...")

    init_cmd = f"sudo -u postgres pgbench -i -s {scale} postgres"
    code, output = run_ssh_command(postgres_ip, init_cmd, timeout=300)
    if code != 0:
        print(f"  Warning: pgbench init may have failed: {output[:500]}")
        return False

    print("  pgbench initialized")
    return True


def run_pgbench(
    benchmark_ip: str,
    postgres_ip: str,
    clients: int = 16,
    threads: int = 4,
    duration: int = 60,
) -> PgBenchResult:
    """Run pgbench from benchmark VM against Postgres VM."""
    print(
        f"  Running pgbench (clients={clients}, duration={duration}s) from benchmark VM..."
    )

    # Connect to postgres VM via its IP
    bench_cmd = (
        f"PGPASSWORD='' pgbench "
        f"-h {postgres_ip} "
        f"-U postgres "
        f"-c {clients} "
        f"-j {threads} "
        f"-T {duration} "
        f"--progress=10 "
        f"postgres 2>&1"
    )

    start_time = time.time()
    try:
        code, output = run_ssh_command(benchmark_ip, bench_cmd, timeout=duration + 60)
    except Exception as e:
        return PgBenchResult(error=str(e))

    elapsed = time.time() - start_time

    if code != 0:
        return PgBenchResult(error=output[:500])

    return parse_pgbench_output(output, elapsed)


def parse_pgbench_output(output: str, duration: float) -> PgBenchResult:
    """Parse pgbench output."""
    result = PgBenchResult(duration_s=duration)

    # Parse TPS: tps = 1234.567890 (without initial connection time)
    tps_match = re.search(r"tps = ([\d.]+) \(without initial connection time\)", output)
    if tps_match:
        result.tps = float(tps_match.group(1))

    # Parse latency: latency average = 1.234 ms
    lat_avg_match = re.search(r"latency average = ([\d.]+) ms", output)
    if lat_avg_match:
        result.latency_avg_ms = float(lat_avg_match.group(1))

    # Parse stddev: latency stddev = 0.567 ms
    lat_std_match = re.search(r"latency stddev = ([\d.]+) ms", output)
    if lat_std_match:
        result.latency_stddev_ms = float(lat_std_match.group(1))

    # Parse transactions
    txn_match = re.search(r"number of transactions actually processed: (\d+)", output)
    if txn_match:
        result.transactions = int(txn_match.group(1))

    if result.tps == 0:
        print(f"  Warning: Could not parse pgbench output. Sample: {output[:500]}...")

    return result


def calculate_cost(infra_config: dict, cloud_config: CloudConfig) -> float:
    """Estimate hourly cost for the configuration."""
    cpu = infra_config.get("cpu", 4)
    ram = infra_config.get("ram_gb", 16)
    disk_size = infra_config.get("disk_size_gb", 100)
    disk_type = infra_config.get("disk_type", "nvme")

    cpu_cost = cpu * cloud_config.cpu_cost
    ram_cost = ram * cloud_config.ram_cost
    disk_cost = disk_size * cloud_config.disk_cost_multipliers.get(disk_type, 0.01)

    return cpu_cost + ram_cost + disk_cost


def save_result(
    result: PgBenchResult,
    infra_config: dict,
    pg_config: dict,
    trial_number: int,
    cloud: str,
    mode: str,
    cloud_config: CloudConfig,
) -> None:
    """Save benchmark result to JSON file."""
    results = load_results(results_file(cloud, mode))

    cost = calculate_cost(infra_config, cloud_config)
    cost_efficiency = result.tps / cost if cost > 0 else 0

    results.append(
        {
            "trial": trial_number,
            "timestamp": datetime.now().isoformat(),
            "cloud": cloud,
            "mode": mode,
            "infra_config": infra_config,
            "pg_config": pg_config,
            "cost_per_hour": cost,
            "cost_efficiency": cost_efficiency,
            "tps": result.tps,
            "latency_avg_ms": result.latency_avg_ms,
            "latency_stddev_ms": result.latency_stddev_ms,
            "transactions": result.transactions,
            "duration_s": result.duration_s,
            "error": result.error,
        }
    )

    save_results(results, results_file(cloud, mode))


def get_metric_value(result: dict, metric: str) -> float:
    """Extract the optimization metric value from a result."""
    if metric == "latency_avg_ms":
        # For latency, we want to minimize, so return negative
        return -result.get("latency_avg_ms", float("inf"))
    return result.get(metric, 0)


def infra_summary(c: dict) -> str:
    """Format infra config as compact string."""
    return f"{c.get('cpu', 0)}cpu/{c.get('ram_gb', 0)}gb/{c.get('disk_type', '?')}"


def pg_summary(c: dict) -> str:
    """Format pg config as compact string."""
    return f"sb={c.get('shared_buffers_pct', 0)}% wm={c.get('work_mem_mb', 0)}mb mc={c.get('max_connections', 0)}"


def format_results(cloud: str, mode: str) -> dict | None:
    """Format benchmark results for display."""
    results = load_results(results_file(cloud, mode))
    if not results:
        return None

    results_sorted = sorted(results, key=lambda x: x.get("tps", 0), reverse=True)

    rows = []
    for r in results_sorted:
        infra = r.get("infra_config", {})
        pg = r.get("pg_config", {})
        rows.append(
            {
                "cpu": infra.get("cpu", 0),
                "ram": infra.get("ram_gb", 0),
                "disk": infra.get("disk_type", "?"),
                "sb_pct": pg.get("shared_buffers_pct", 0),
                "wm_mb": pg.get("work_mem_mb", 0),
                "mc": pg.get("max_connections", 0),
                "tps": r.get("tps", 0),
                "lat": r.get("latency_avg_ms", 0),
                "cost": r.get("cost_per_hour", 0),
                "eff": r.get("cost_efficiency", 0),
            }
        )

    best_tps = max(results, key=lambda x: x.get("tps", 0))
    best_lat = min(results, key=lambda x: x.get("latency_avg_ms", float("inf")))
    best_eff = max(results, key=lambda x: x.get("cost_efficiency", 0))

    return {
        "cloud": cloud,
        "mode": mode,
        "rows": rows,
        "best": {
            "tps": {
                "value": best_tps.get("tps", 0),
                "infra": infra_summary(best_tps.get("infra_config", {})),
                "pg": pg_summary(best_tps.get("pg_config", {})),
            },
            "latency": {
                "value": best_lat.get("latency_avg_ms", 0),
                "infra": infra_summary(best_lat.get("infra_config", {})),
                "pg": pg_summary(best_lat.get("pg_config", {})),
            },
            "efficiency": {
                "value": best_eff.get("cost_efficiency", 0),
                "infra": infra_summary(best_eff.get("infra_config", {})),
                "pg": pg_summary(best_eff.get("pg_config", {})),
            },
        },
    }


def show_results(cloud: str, mode: str) -> None:
    """Display benchmark results."""
    data = format_results(cloud, mode)
    if not data:
        print(f"No results found for {cloud}/{mode}")
        return

    print(f"\n{'=' * 100}")
    print(f"Postgres Benchmark Results - {cloud.upper()} [{mode}]")
    print(f"{'=' * 100}")

    print(
        f"{'#':>3} {'CPU':>4} {'RAM':>4} {'Disk':<5} {'SB%':>4} {'WM':>5} {'MC':>4} "
        f"{'TPS':>10} {'Lat(ms)':>8} {'$/hr':>6} {'Eff':>8}"
    )
    print("-" * 100)

    for i, r in enumerate(data["rows"], 1):
        print(
            f"{i:>3} {r['cpu']:>4} {r['ram']:>4} {r['disk']:<5} {r['sb_pct']:>4} {r['wm_mb']:>5} {r['mc']:>4} "
            f"{r['tps']:>10.1f} {r['lat']:>8.2f} {r['cost']:>6.2f} {r['eff']:>8.0f}"
        )

    print("-" * 100)
    print(f"Total: {len(data['rows'])} results")

    best = data["best"]
    print(
        f"\nBest by TPS:        {best['tps']['value']:>10.1f} {'TPS':<8} [{best['tps']['infra']}] [{best['tps']['pg']}]"
    )
    print(
        f"Best by latency:    {best['latency']['value']:>10.2f} {'ms':<8} [{best['latency']['infra']}] [{best['latency']['pg']}]"
    )
    print(
        f"Best by efficiency: {best['efficiency']['value']:>10.0f} {'TPS/$/hr':<8} [{best['efficiency']['infra']}] [{best['efficiency']['pg']}]"
    )


def export_results_md(cloud: str, mode: str, output_path: Path | None = None) -> None:
    """Export benchmark results to markdown."""
    data = format_results(cloud, mode)
    if not data:
        print(f"No results found for {cloud}/{mode}")
        return

    if output_path is None:
        output_path = RESULTS_DIR / f"RESULTS_{cloud.upper()}_{mode.upper()}.md"

    lines = [
        f"# Postgres Benchmark Results - {cloud.upper()} [{mode}]",
        "",
        f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        "",
        "## Results",
        "",
        "| # | CPU | RAM | Disk | SB% | WM | MC | TPS | Lat(ms) | $/hr | Efficiency |",
        "|--:|----:|----:|------|----:|---:|---:|----:|--------:|-----:|-----------:|",
    ]

    for i, r in enumerate(data["rows"], 1):
        lines.append(
            f"| {i} | {r['cpu']} | {r['ram']} | {r['disk']} | {r['sb_pct']} | {r['wm_mb']} | {r['mc']} "
            f"| {r['tps']:.1f} | {r['lat']:.2f} | {r['cost']:.2f} | {r['eff']:.0f} |"
        )

    best = data["best"]
    lines.extend(
        [
            "",
            "## Best Configurations",
            "",
            f"- **Best by TPS:** {best['tps']['value']:.1f} TPS — `{best['tps']['infra']}` `{best['tps']['pg']}`",
            f"- **Best by latency:** {best['latency']['value']:.2f}ms — `{best['latency']['infra']}` `{best['latency']['pg']}`",
            f"- **Best by efficiency:** {best['efficiency']['value']:.0f} TPS/$/hr — `{best['efficiency']['infra']}` `{best['efficiency']['pg']}`",
            "",
        ]
    )

    output_path.write_text("\n".join(lines))
    print(f"Results exported to {output_path}")


def objective_infra(
    trial: optuna.Trial,
    cloud: str,
    cloud_config: CloudConfig,
    metric: str = "tps",
) -> float:
    """Objective function for infrastructure optimization."""
    space = get_infra_search_space(cloud)

    infra_config = {
        "mode": trial.suggest_categorical("mode", space["mode"]),
        "cpu": trial.suggest_categorical("cpu", space["cpu"]),
        "ram_gb": trial.suggest_categorical("ram_gb", space["ram_gb"]),
        "disk_type": trial.suggest_categorical("disk_type", space["disk_type"]),
        "disk_size_gb": trial.suggest_categorical(
            "disk_size_gb", space["disk_size_gb"]
        ),
    }

    # Use reasonable default Postgres config
    ram_gb = infra_config["ram_gb"]
    pg_config = {
        "shared_buffers_pct": 25,
        "effective_cache_size_pct": 75,
        "work_mem_mb": 64,
        "maintenance_work_mem_mb": 256,
        "max_connections": 100,
        "random_page_cost": 1.1,
        "effective_io_concurrency": 200,
        "wal_buffers_mb": 64,
        "max_wal_size_gb": 2,
        "checkpoint_completion_target": 0.9,
        "max_worker_processes": min(infra_config["cpu"], 8),
        "max_parallel_workers_per_gather": min(infra_config["cpu"] // 2, 4),
    }

    print(f"\n{'=' * 60}")
    print(f"Trial {trial.number} [infra]: {infra_config}")
    print(f"{'=' * 60}")

    # Check cache
    cached = find_cached_result(infra_config, pg_config, cloud, "infra")
    if cached:
        cached_value = get_metric_value(cached, metric)
        print(f"  Using cached result: {cached_value:.2f} ({metric})")
        return cached_value

    # Destroy and recreate VM with new specs
    print("  Destroying previous VM...")
    destroy_all(cloud_config.terraform_dir, cloud_config.name)
    time.sleep(5)

    # Create VMs
    try:
        benchmark_ip, postgres_ip = ensure_infra(cloud_config, infra_config)
    except Exception as e:
        print(f"  Failed to create infrastructure: {e}")
        raise optuna.TrialPruned("Infrastructure creation failed")

    # Configure Postgres
    mode = infra_config.get("mode", "single")
    if not reconfigure_postgres(postgres_ip, pg_config, ram_gb, mode):
        raise optuna.TrialPruned("Postgres config failed")

    # Initialize pgbench (scale based on RAM)
    scale = max(50, ram_gb * 10)
    if not initialize_pgbench(postgres_ip, scale=scale):
        raise optuna.TrialPruned("pgbench init failed")

    # Run benchmark
    result = run_pgbench(
        benchmark_ip, postgres_ip, clients=infra_config["cpu"] * 4, duration=60
    )

    if result.error:
        print(f"  Benchmark failed: {result.error}")
        raise optuna.TrialPruned(result.error)

    print(f"  Result: {result.tps:.1f} TPS, {result.latency_avg_ms:.2f}ms latency")

    # Save result
    save_result(
        result, infra_config, pg_config, trial.number, cloud, "infra", cloud_config
    )

    return get_metric_value(
        {
            "tps": result.tps,
            "latency_avg_ms": result.latency_avg_ms,
            "cost_efficiency": result.tps / calculate_cost(infra_config, cloud_config),
        },
        metric,
    )


def objective_config(
    trial: optuna.Trial,
    cloud: str,
    cloud_config: CloudConfig,
    benchmark_ip: str,
    postgres_ip: str,
    infra_config: dict,
    metric: str = "tps",
) -> float:
    """Objective function for Postgres config optimization (fixed host)."""
    ram_gb = infra_config["ram_gb"]
    space = get_config_search_space(ram_gb)

    pg_config = {
        "shared_buffers_pct": trial.suggest_categorical(
            "shared_buffers_pct", space["shared_buffers_pct"]
        ),
        "effective_cache_size_pct": trial.suggest_categorical(
            "effective_cache_size_pct", space["effective_cache_size_pct"]
        ),
        "work_mem_mb": trial.suggest_categorical("work_mem_mb", space["work_mem_mb"]),
        "maintenance_work_mem_mb": trial.suggest_categorical(
            "maintenance_work_mem_mb", space["maintenance_work_mem_mb"]
        ),
        "max_connections": trial.suggest_categorical(
            "max_connections", space["max_connections"]
        ),
        "random_page_cost": trial.suggest_categorical(
            "random_page_cost", space["random_page_cost"]
        ),
        "effective_io_concurrency": trial.suggest_categorical(
            "effective_io_concurrency", space["effective_io_concurrency"]
        ),
        "wal_buffers_mb": trial.suggest_categorical(
            "wal_buffers_mb", space["wal_buffers_mb"]
        ),
        "max_wal_size_gb": trial.suggest_categorical(
            "max_wal_size_gb", space["max_wal_size_gb"]
        ),
        "checkpoint_completion_target": trial.suggest_categorical(
            "checkpoint_completion_target", space["checkpoint_completion_target"]
        ),
        "max_worker_processes": trial.suggest_categorical(
            "max_worker_processes", space["max_worker_processes"]
        ),
        "max_parallel_workers_per_gather": trial.suggest_categorical(
            "max_parallel_workers_per_gather", space["max_parallel_workers_per_gather"]
        ),
    }

    print(f"\n{'=' * 60}")
    print(f"Trial {trial.number} [config]: {pg_summary(pg_config)}")
    print(f"{'=' * 60}")

    # Check cache
    cached = find_cached_result(infra_config, pg_config, cloud, "config")
    if cached:
        cached_value = get_metric_value(cached, metric)
        print(f"  Using cached result: {cached_value:.2f} ({metric})")
        return cached_value

    # Reconfigure Postgres (no VM recreation)
    mode = infra_config.get("mode", "single")
    if not reconfigure_postgres(postgres_ip, pg_config, ram_gb, mode):
        raise optuna.TrialPruned("Postgres config failed")

    # Run benchmark
    result = run_pgbench(
        benchmark_ip, postgres_ip, clients=infra_config["cpu"] * 4, duration=60
    )

    if result.error:
        print(f"  Benchmark failed: {result.error}")
        raise optuna.TrialPruned(result.error)

    print(f"  Result: {result.tps:.1f} TPS, {result.latency_avg_ms:.2f}ms latency")

    # Save result
    save_result(
        result, infra_config, pg_config, trial.number, cloud, "config", cloud_config
    )

    return get_metric_value(
        {
            "tps": result.tps,
            "latency_avg_ms": result.latency_avg_ms,
            "cost_efficiency": result.tps / calculate_cost(infra_config, cloud_config),
        },
        metric,
    )


def main():
    parser = argparse.ArgumentParser(
        description="Postgres configuration optimizer",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Tune VM specs
  uv run python postgres-optimizer/optimizer.py --cloud timeweb --mode infra --trials 10

  # Tune postgresql.conf on 8cpu/32gb host
  uv run python postgres-optimizer/optimizer.py --cloud timeweb --mode config --cpu 8 --ram 32 --trials 50

  # Full optimization
  uv run python postgres-optimizer/optimizer.py --cloud timeweb --mode full --trials 20
""",
    )

    parser.add_argument("--cloud", required=True, choices=["selectel", "timeweb"])
    parser.add_argument("--mode", choices=["infra", "config", "full"], default="config")
    parser.add_argument("--trials", type=int, default=20)
    parser.add_argument("--metric", choices=list(METRICS.keys()), default="tps")

    # Fixed host settings for config mode
    parser.add_argument("--cpu", type=int, default=4, help="CPU cores for config mode")
    parser.add_argument("--ram", type=int, default=16, help="RAM GB for config mode")

    parser.add_argument(
        "--no-destroy", action="store_true", help="Don't destroy infra at end"
    )
    parser.add_argument(
        "--show-results", action="store_true", help="Show results and exit"
    )
    parser.add_argument(
        "--export-md", action="store_true", help="Export to markdown and exit"
    )

    args = parser.parse_args()

    cloud_config = get_cloud_config(args.cloud)
    mode = Mode(args.mode)

    # Handle display modes
    if args.show_results:
        show_results(args.cloud, args.mode)
        return

    if args.export_md:
        export_results_md(args.cloud, args.mode)
        return

    print(f"\nPostgres Optimizer - {args.cloud.upper()} [{mode.value}]")
    print(f"Metric: {args.metric} - {METRICS[args.metric]}")
    print(f"Trials: {args.trials}")

    study: optuna.Study | None = None

    try:
        if mode == Mode.INFRA:
            # Infrastructure optimization
            study = optuna.create_study(
                study_name=f"postgres-{args.cloud}-infra",
                storage=f"sqlite:///{STUDY_DB}",
                load_if_exists=True,
                direction="maximize" if args.metric != "latency_avg_ms" else "minimize",
                sampler=TPESampler(seed=42),
            )

            study.optimize(
                lambda trial: objective_infra(
                    trial, args.cloud, cloud_config, args.metric
                ),
                n_trials=args.trials,
                catch=(optuna.TrialPruned,),
            )

        elif mode == Mode.CONFIG:
            # Config optimization on fixed host
            infra_config = {
                "cpu": args.cpu,
                "ram_gb": args.ram,
                "disk_type": "nvme" if args.cloud == "timeweb" else "fast",
                "disk_size_gb": 100,
            }

            print(f"Fixed host: {infra_summary(infra_config)}")

            # Ensure VMs exist
            benchmark_ip, postgres_ip = ensure_infra(cloud_config, infra_config)
            print(f"\nPostgres VM IP: {postgres_ip}")
            print(f"Benchmark VM IP: {benchmark_ip}")

            # Initialize pgbench once
            scale = max(50, args.ram * 10)
            initialize_pgbench(postgres_ip, scale=scale)

            study = optuna.create_study(
                study_name=f"postgres-{args.cloud}-config",
                storage=f"sqlite:///{STUDY_DB}",
                load_if_exists=True,
                direction="maximize" if args.metric != "latency_avg_ms" else "minimize",
                sampler=TPESampler(seed=42),
            )

            study.optimize(
                lambda trial: objective_config(
                    trial,
                    args.cloud,
                    cloud_config,
                    benchmark_ip,
                    postgres_ip,
                    infra_config,
                    args.metric,
                ),
                n_trials=args.trials,
                catch=(optuna.TrialPruned,),
            )

        elif mode == Mode.FULL:
            # Full optimization: infra first, then config
            print("\n=== Phase 1: Infrastructure optimization ===")

            study_infra = optuna.create_study(
                study_name=f"postgres-{args.cloud}-full-infra",
                storage=f"sqlite:///{STUDY_DB}",
                load_if_exists=True,
                direction="maximize",
                sampler=TPESampler(seed=42),
            )

            infra_trials = max(5, args.trials // 3)
            study_infra.optimize(
                lambda trial: objective_infra(
                    trial, args.cloud, cloud_config, args.metric
                ),
                n_trials=infra_trials,
                catch=(optuna.TrialPruned,),
            )

            # Get best infra config
            best_infra = study_infra.best_params
            infra_config = {
                "cpu": best_infra["cpu"],
                "ram_gb": best_infra["ram_gb"],
                "disk_type": best_infra["disk_type"],
                "disk_size_gb": best_infra["disk_size_gb"],
            }

            print("\n=== Phase 2: Config optimization on best host ===")
            print(f"Best host: {infra_summary(infra_config)}")

            # Create VMs with best infra
            destroy_all(cloud_config.terraform_dir, cloud_config.name)
            benchmark_ip, postgres_ip = ensure_infra(cloud_config, infra_config)

            scale = max(50, infra_config["ram_gb"] * 10)
            initialize_pgbench(postgres_ip, scale=scale)

            study_config = optuna.create_study(
                study_name=f"postgres-{args.cloud}-full-config",
                storage=f"sqlite:///{STUDY_DB}",
                load_if_exists=True,
                direction="maximize" if args.metric != "latency_avg_ms" else "minimize",
                sampler=TPESampler(seed=42),
            )

            config_trials = args.trials - infra_trials
            study_config.optimize(
                lambda trial: objective_config(
                    trial,
                    args.cloud,
                    cloud_config,
                    benchmark_ip,
                    postgres_ip,
                    infra_config,
                    args.metric,
                ),
                n_trials=config_trials,
                catch=(optuna.TrialPruned,),
            )

            study = study_config  # For final output

        # Print results
        print(f"\n{'=' * 60}")
        print(f"OPTIMIZATION COMPLETE ({args.cloud.upper()} [{mode.value}])")
        print(f"{'=' * 60}")

        if study is not None:
            try:
                best = study.best_trial
                print(f"Best trial: {best.number}")
                print(f"Best params: {best.params}")
                print(f"Best {args.metric}: {best.value:.2f}")
            except ValueError:
                print("No successful trials completed")
        else:
            print("No study was created")

        # Auto-export results
        export_results_md(args.cloud, args.mode)
        print(
            f"\nResults exported to RESULTS_{args.cloud.upper()}_{args.mode.upper()}.md"
        )

    finally:
        if not args.no_destroy:
            destroy_all(cloud_config.terraform_dir, cloud_config.name)
        else:
            print("\n--no-destroy specified, keeping infrastructure.")


if __name__ == "__main__":
    main()
